{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column|Description|\n",
    "|:------:|:---:|\n",
    "|Created|Time of ticket registration|\n",
    "|CancelTime|When the passenger canceled the ticket|\n",
    "|DepartureTime|Time of departure|\n",
    "|BillID|Purchase ID|\n",
    "|TicketID|Ticket ID|\n",
    "|ReserveStatus|Customer payment status|\n",
    "|UserID|User ID|\n",
    "|Male|Whether the ticket belongs to a woman or a man|\n",
    "|Price|Undiscounted ticket price|\n",
    "|CouponDiscount|Discount that the person applied to the ticket|\n",
    "|From|Origin of the trip|\n",
    "|To|Destination of the trip|\n",
    "|Domestic|Whether the trip is domestic or international|\n",
    "|VehicleType|Identifies vehicle details|\n",
    "|VehicleClass|Whether the vehicle is first class or not|\n",
    "|Vehicle|Vehicle type|\n",
    "|HashPassportNumber_p|Hashed passport number|\n",
    "|HashEmail|Hashed Email|\n",
    "|BuyerMobile|Hashed Mobile Number|\n",
    "|NationalCode|Hashed National Number|\n",
    "|TripReason|Reason for Trip|\n",
    "|Cancel|Whether the Ticket is Cancelled or Not|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit = pd.read_csv('data/mrbilit_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit = mrbilit.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.select_dtypes(include=['int64', 'float64']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.select_dtypes(exclude=['int64', 'float64']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(mrbilit, x=\"Cancel\", color=\"Male\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(mrbilit, x=\"Cancel\", color=\"VehicleClass\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(mrbilit, x=\"Cancel\", color=\"TripReason\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(mrbilit, x=\"Vehicle\", color=\"Cancel\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(mrbilit, x=\"Domestic\", color=\"Cancel\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit[['Created', 'DepartureTime']] = mrbilit[['Created', 'DepartureTime']].apply(pd.to_datetime)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit_month_cancels = mrbilit[['DepartureTime', 'Cancel']]\n",
    "mrbilit_month_cancels['year'] = mrbilit_month_cancels['DepartureTime'].dt.year\n",
    "mrbilit_month_cancels['month'] = mrbilit_month_cancels['DepartureTime'].dt.month\n",
    "not_canceld = mrbilit_month_cancels.groupby(['Cancel', 'year', 'month']).count().unstack().iloc[0]\n",
    "canceld = mrbilit_month_cancels.groupby(['Cancel', 'year', 'month']).count().unstack().iloc[2]\n",
    "all_tickets = canceld.fillna(0) + not_canceld\n",
    "df_line_cancels_by_month = pd.DataFrame({\n",
    "    'month': np.arange(1, 13),\n",
    "    'all': all_tickets.values,\n",
    "    'canceld': canceld.values\n",
    "})\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_line_cancels_by_month['month'], y=df_line_cancels_by_month['all'], name='all',\n",
    "                         line=dict(color='firebrick', width=4)))\n",
    "fig.add_trace(go.Scatter(x=df_line_cancels_by_month['month'], y=df_line_cancels_by_month['canceld'], name = 'canceld',\n",
    "                         line=dict(color='royalblue', width=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit['TimeDiff'] = (mrbilit['DepartureTime'] - mrbilit['Created']).dt.days\n",
    "df_timediff = mrbilit.groupby(['Cancel', 'TimeDiff'])['BillID'].count().unstack()\n",
    "df_timediff = df_timediff.iloc[: , 0:50]\n",
    "y_all = df_timediff.iloc[0].fillna(0) + df_timediff.iloc[1].fillna(0)\n",
    "fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=df_timediff.columns, y=y_all, name='all',\n",
    "#                          line=dict(color='firebrick', width=4)))\n",
    "# fig.add_trace(go.Scatter(x=df_timediff.columns, y=df_timediff.iloc[1], name = 'canceld',\n",
    "#                          line=dict(color='royalblue', width=4)))\n",
    "fig.add_trace(go.Scatter(x=df_timediff.columns, y=df_timediff.iloc[1]/y_all, name = 'نسبت',\n",
    "                         line=dict(color='red', width=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit['To-Count'] = mrbilit.groupby('To')['BillID'].transform(lambda x: len(x))\n",
    "fig = px.histogram(mrbilit.sort_values(by='To-Count') , x=\"To\", color=\"Cancel\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit['has_discount'] = mrbilit.CouponDiscount > 0\n",
    "fig = px.histogram(mrbilit , x=\"has_discount\", color=\"Cancel\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit['TicketPerOrder'] = mrbilit.groupby('BillID').TicketID.transform('count')\n",
    "fig = px.histogram(mrbilit , x=\"TicketPerOrder\", color=\"Cancel\", title=\"\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_family(x):\n",
    "    if len(x) > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrbilit.groupby('Cancel')['Created'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.selected_cols = ['Created', 'DepartureTime', 'BillID', 'TicketID', 'ReserveStatus', \n",
    "                'Male', 'Price', 'CouponDiscount', 'From', 'To', 'Domestic',\n",
    "                'VehicleClass', 'Vehicle', 'BuyerMobile', 'TripReason']\n",
    "\n",
    "        self.final_features = ['ReserveStatus',\n",
    "       'Male', 'Domestic', 'Vehicle', 'TripReason',\n",
    "       'TimeDiff', 'TicketPerOrder', 'is_with_familiy', 'net_price',\n",
    "       'From_encoded', 'To_encoded', 'has_discount']\n",
    "        \n",
    "\n",
    "    def select_useful_cols (self) : \n",
    "        self.df = self.dataset[self.selected_cols]\n",
    "\n",
    "\n",
    "    def is_with_family(self):\n",
    "        self.df['is_with_familiy'] = self.df.groupby('BillID')['Male'].transform(count_family)\n",
    "\n",
    "\n",
    "    def datetime_cols (self) : \n",
    "        self.df[['Created', 'DepartureTime']] = self.df[['Created', 'DepartureTime']].apply(pd.to_datetime)        \n",
    "        self.df['TimeDiff'] = (self.df['DepartureTime'] - self.df['Created']).dt.days\n",
    "        self.df.drop(['Created', 'DepartureTime'], axis=1, inplace=True)\n",
    "    \n",
    "    def ticket_per_order (self) : \n",
    "        self.df['TicketPerOrder'] = self.df.groupby('BillID').TicketID.transform('count')\n",
    "    \n",
    "    def handle_monetary (self) : \n",
    "        self.df['net_price'] = self.df.Price - self.df.CouponDiscount\n",
    "        self.df['net_price'] /= self.df['net_price'].max()\n",
    "        self.df['has_discount'] = self.df.CouponDiscount > 0\n",
    "\n",
    "    \n",
    "    def encode_cities (self, is_train) : \n",
    "        if is_train : \n",
    "            cities = list(set(self.df.From.unique().tolist()).union(set(self.df.To.unique().tolist())))\n",
    "            self.city_encoder = LabelEncoder().fit(cities)\n",
    "        try : \n",
    "            city2idx = dict(zip(self.city_encoder.classes_, self.city_encoder.transform(self.city_encoder.classes_)))\n",
    "            \n",
    "            self.df['From_encoded'] = self.df.From.map(city2idx).fillna(-1).astype(int)\n",
    "            self.df['To_encoded'] = self.df.To.map(city2idx).fillna(-1).astype(int)\n",
    "            \n",
    "            self.df.drop(['To', 'From'], axis=1, inplace=True)\n",
    "        \n",
    "        except KeyError as exc:  \n",
    "            raise Exception('Something went wrong. Maybe this class is used for test date before fit on train data!')\n",
    "    \n",
    "    def encode_categorical_cols (self, is_train) : \n",
    "        if is_train : \n",
    "            self.cat_cols = self.df.select_dtypes(exclude=['int','float']).columns\n",
    "            self.cat_les = {}\n",
    "\n",
    "            for col in self.cat_cols : \n",
    "                le = LabelEncoder().fit(self.df[col])\n",
    "                self.cat_les.update({col:le})\n",
    "        try:\n",
    "            for col in self.cat_cols :\n",
    "                self.df[col] = self.cat_les[col].transform(self.df[col])\n",
    "        except Exception as exc: \n",
    "            raise Exception('Something went wrong. Maybe this class is used for test date before fit on train data!')\n",
    "        \n",
    "    def select_final_features(self):\n",
    "        self.df = self.df[self.final_features]\n",
    "\n",
    "        \n",
    "    def transform(self, dataset:pd.DataFrame, is_train=True) : \n",
    "        self.dataset = dataset.copy()\n",
    "        \n",
    "        self.select_useful_cols()\n",
    "        self.datetime_cols()\n",
    "        self.ticket_per_order()\n",
    "        self.is_with_family()\n",
    "        self.handle_monetary()\n",
    "        self.encode_cities(is_train)\n",
    "        self.encode_categorical_cols(is_train)\n",
    "        self.select_final_features()\n",
    "        \n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mrbilit.drop(columns='Cancel')\n",
    "y = mrbilit['Cancel']\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.01, shuffle=True, stratify=y)\n",
    "\n",
    "x_train, x_val, y_train, y_val  = train_test_split(x_train_val, y_train_val, test_size=0.05, shuffle=True, stratify=y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "\n",
    "X_train = preprocessor.transform(x_train, is_train = True)\n",
    "X_val = preprocessor.transform(x_val, is_train = False)\n",
    "X_test = preprocessor.transform(x_test, is_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': uniform(1, 10),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    scoring='f1',\n",
    "    cv=4,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))\n",
    "models.append({\n",
    "    'name': 'XGBClassifier',\n",
    "    'f1_score': f1_score(y_val, y_pred),\n",
    "    'model' : model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 200, 10),       \n",
    "    'max_depth': [None] + list(np.arange(10, 50, 5)),  \n",
    "    'min_samples_split': np.arange(2, 20, 2),     \n",
    "    'min_samples_leaf': np.arange(1, 10, 1),      \n",
    "    'max_features': ['sqrt', 'log2', None],       \n",
    "    'bootstrap': [True, False]                   \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  \n",
    "    scoring='f1',\n",
    "    cv=4,       \n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1   \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))\n",
    "models.append({\n",
    "    'name': 'RandomForest',\n",
    "    'f1_score': f1_score(y_val, y_pred),\n",
    "    'model' : model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 300, 10),            \n",
    "    'learning_rate': np.linspace(0.01, 0.2, 20),     \n",
    "    'max_depth': np.arange(3, 15, 1),                 \n",
    "    'min_samples_split': np.arange(2, 20, 2),         \n",
    "    'min_samples_leaf': np.arange(1, 10, 1),          \n",
    "    'subsample': np.linspace(0.6, 1.0, 5),            \n",
    "    'max_features': ['sqrt', 'log2', None]            \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10, \n",
    "    scoring='f1',\n",
    "    cv=4,       \n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1   \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))\n",
    "models.append({\n",
    "    'name': 'GradientBoosting',\n",
    "    'f1_score': f1_score(y_val, y_pred),\n",
    "    'model' : model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm  = LGBMClassifier(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': np.arange(20, 150, 10),             \n",
    "    'max_depth': [-1] + list(np.arange(3, 15, 1)),   \n",
    "    'learning_rate': np.linspace(0.01, 0.2, 20),\n",
    "    'n_estimators': np.arange(50, 500, 50),           \n",
    "    'min_child_samples': np.arange(10, 100, 10),     \n",
    "    'subsample': np.linspace(0.6, 1.0, 5),            \n",
    "    'colsample_bytree': np.linspace(0.6, 1.0, 5),     \n",
    "    'reg_alpha': np.linspace(0, 1.0, 10),            \n",
    "    'reg_lambda': np.linspace(0, 1.0, 10),           \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  \n",
    "    scoring='f1',\n",
    "    cv=4,       \n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1   \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))\n",
    "models.append({\n",
    "    'name': 'LGBM',\n",
    "    'f1_score': f1_score(y_val, y_pred),\n",
    "    'model' : model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 50),             \n",
    "    'weights': ['uniform', 'distance'],         \n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  \n",
    "    'p': np.arange(1, 4)                        \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  \n",
    "    scoring='f1',\n",
    "    cv=4,       \n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1   \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred))\n",
    "models.append({\n",
    "    'name': 'KNeighbors',\n",
    "    'f1_score': f1_score(y_val, y_pred),\n",
    "    'model' : model\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# svm = SVC(random_state=42)\n",
    "\n",
    "# param_dist = {\n",
    "#     'kernel': ['linear', 'poly', 'sigmoid', 'precomputed'],\n",
    "#     'degree': np.arange(2, 6)\n",
    "# }\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=svm,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=5,  \n",
    "#     scoring='accuracy',\n",
    "#     cv=4,       \n",
    "#     random_state=42,\n",
    "#     verbose=2,\n",
    "#     n_jobs=-1   \n",
    "# )\n",
    "\n",
    "# random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = random_search.best_estimator_\n",
    "# y_pred = model.predict(X_val)\n",
    "# print(classification_report(y_val, y_pred))\n",
    "# print(f1_score(y_val, y_pred))\n",
    "# models.append({\n",
    "#     'name': 'SVC',\n",
    "#     'f1_score': f1_score(y_val, y_pred),\n",
    "#     'model' : model\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort(key=lambda x: x['f1_score'])\n",
    "for model in models:\n",
    "    print(model['name'], model['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model['name'], f1_score(model['model'].predict(X_test), y_test))\n",
    "    if model['name'] not in ['SVC', 'KNeighbors']:\n",
    "        l = []\n",
    "        for i in range(len(X_train.columns)):\n",
    "            l.append([X_train.columns[i], round(model['model'].feature_importances_[i], 3)])\n",
    "        print(sorted(l, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    print('-----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
